{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95496219",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to AI Explainability - part 5: SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecff2d6",
   "metadata": {},
   "source": [
    "## Context\n",
    "In [part 4](./AIX_Intro-B4.ipynb) we went through the theoretical foundation of Shapley values. In this part we focus on application of Shapley values in local explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a9fc8",
   "metadata": {},
   "source": [
    "# Additive feature attribution methods\n",
    "Additive feature attribution methods have a an explanation model that is a linear function of binary variables.\n",
    "$$\n",
    "\\large{\n",
    "g(z^{\\prime}) = \\phi_0 + \\sum_{i=1}^M\\phi_iz^\\prime_i\n",
    "}\n",
    "$$\n",
    "where $z^\\prime \\in \\{0,1\\}^M$, and $M$ is the number of simplified input features, and $\\phi_i \\in \\mathbb{R}$.\n",
    "\n",
    "Family of explanation models that matching definition of additive feature attribution models assign an attribution $\\phi_i$ to each feature and then sum up the effects of all features to approximate the original model $f$ over a specific input. \n",
    "In the following sections we explain such models as well as their practical application. \n",
    "\n",
    "Additive feature attribution methods have *desirable* properties that uniquely determine additive feature attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a1b17",
   "metadata": {},
   "source": [
    "### Local accuracy\n",
    "Local accuracy requires the explanation model $g$ to at least match the output of original model $f$ for the simplified input $x^\\prime$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dda6e",
   "metadata": {},
   "source": [
    "### Missingness\n",
    "features that are missing from the simplified input, which describe in binary terms where or or not a feature is present, must have no impact, or $x_i^\\prime = 0 \\Rightarrow \\phi_i=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db83ae",
   "metadata": {},
   "source": [
    "### Consistency\n",
    "If a model changes in a way that impact of a feature increases, the attribution should never decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec3a43",
   "metadata": {},
   "source": [
    "## SHAP for Explainability\n",
    "So far we have seen that Shapley values provide a ***unique*** and ***fair*** way to distribute the payout amongst the players in a collaborative and competitive game. We have also seen references to feature attribution methods. Additionally we know from the context that SHAP is an additive feature attribution methods and thus has a linear local explanation based on simplified input.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206112a8",
   "metadata": {},
   "source": [
    "The authors of the SHAP paper[10] have provided an equation that is akin to Shapley values equation and prove that there is unique solution to that equation, which is composed of a conditional expectation function of the original model that is being explained. In this case the explanation model should satisfy the three properties of the additive feature attribution methods, which again are akin to three axioms of for Shapley theorm. The equation that needs to be answered is defined as:\n",
    "$$\n",
    "\\large{\n",
    "\\phi_i(f,x) = \\sum_{z^\\prime \\subseteq x^\\prime} \\frac{|z^\\prime|!(M-|z^\\prime|-1)!}{M!} \\left[ f_x(z^\\prime)-f_x(z^\\prime \\setminus i) \\right]\n",
    "}\n",
    "$$\n",
    "where $|z|$ is the number of non-zero entries in z, and $z \\subseteq x$ represents all $z$ vectors where the non-zero entries are a subset of the non-zero entries in $x$. $f_x(z\\prime) = E \\left[ f(z) | z_S\\right]$ where $S$ is a set of non-zero indexes in $z^\\prime$\n",
    "\n",
    "Comparing this equation to shapley values, we observe that $\\phi = \\{\\phi_i:\\ 0\\ \\leq i\\ \\leq M\\}$. $f$, the original model is the payout function and $x$ denotes the grand coalition. finally $z$ is permutations of possible coalitions, which we saw in the simple example s we created those permutations. \n",
    "\n",
    "<figure>\n",
    "    <img img align=\"center\"  src='data/SHAP.png'>\n",
    "    <figcaption>\n",
    "        Figure 1: $[E(f(z)]$, corresponding to $\\phi_0$ is the expectation of the model over input features. Then as we build all the possible alliances $x_1 = \\rightarrow x_{1,2} \\rightarrow x_{1,2,3}$ we get the conditional expectation  of $f(x)$ for the newly added feature. We should keep in mind that the solution is dependent on the order the population is generated in the cases where the features are dependent on one another or the model $f$ is non-linear.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313a986",
   "metadata": {},
   "source": [
    "## Approximating SHAP values\n",
    "Instead of heavy and complicated computation of SHAP values, we can approximate them with some accuracy. Authors of [10] has proposed two model agnostic methods of which we only focus on Kernel SHAP, which is a combination of LIME and Shapley values. In next postings, we describe DeepLift and DeepSHAP, which combined DeepLift an Shapley values and is model specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39887d7f",
   "metadata": {},
   "source": [
    "### Kernel SHAP\n",
    "We remember creating a LIME explanator resulted in solving the optimization \n",
    "\n",
    "$$\n",
    "\\large{\\xi(x) = \\text{arg} \\min\\limits_{g \\in G} \\mathcal{L}(f,g,\\Pi_x) + \\Omega(g)}\n",
    "$$.\n",
    "\n",
    "We should also note that choosing loss function and $\\Omega$, and weighting kernel, $\\Pi$ to solve the LIME optimization equation are empirical and based on heuristic methods, thus the explanations can vary depending on how we choose those hyperparameters. The question is whether we could do better and have a consistent and locally accurate solution. The answer is yes. Since LIME is a Additive feature attribution method, Shapley values are the unique solution to the problem of finding an explanator where the desired properties, local accuracy, missingness, and consistency are satisfied; therefore the question of consistency and local accuracy for LIME comes down to find the Shapley values to to find the hyperparameters $\\Omega$, $\\mathcal{L}$, and $\\Pi$ and avoid heuristic methods.\n",
    "The authors propose and prove the following values for the huperparameters to be the unique solution that satisfies the desired properties:\n",
    "$$\n",
    "\\large{\n",
    "\\Omega(g)=0\n",
    "}\n",
    "$$\n",
    "As seen earlier $\\Omega$ represented complexity such as depth of a tree or the number of non-zero weights. This is heuristic and arbitrary. By setting it to $0$, we become independent of such arbitrary choice of complexity.\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\Pi_{x^\\prime}(z^\\prime)=\\frac{M-1}{\\left(M choose |z^\\prime| \\right )|z^\\prime|\\left( |M|-|z^\\prime|\\right )}\n",
    "}\n",
    "$$\n",
    "The apparent similarity to weighting of Shapley values is obvious.\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\mathcal{L}(f,g,\\Pi_{x^\\prime}) = \\sum_{z^\\prime \\in z} \\left[ f(h_x^{-1}(z^\\prime))-g(z^\\prime)\\right ]^2\\Pi_{x^\\prime(z^\\prime)}\n",
    "}\n",
    "$$\n",
    "$\\mathcal{L}$ corresponds to the weighted average of the conditional expectations in the SHAP method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6dd323",
   "metadata": {},
   "source": [
    "## Implementation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c00eb",
   "metadata": {},
   "source": [
    "#### Loading an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d9969",
   "metadata": {},
   "source": [
    "```python\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "pil_img = Image.open(\"data/boyz2.jpeg\")\n",
    "pil_img.convert(\"RGB\")\n",
    "\n",
    "resize_transform  = transforms.Resize((224,224))\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "pil_img = resize_transform(pil_img)\n",
    "plt.imshow(pil_img)\n",
    "pil_img = tensor_transform(pil_img)#.numpy()\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                std=[0.229, 0.224, 0.225])  \n",
    "#pil_img = normalize(pil_img)\n",
    "pil_img = np.transpose(pil_img, (1,2,0))\n",
    "pil_img = pil_img * 255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d27c6",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img img align=\"center\"  width=\"224\" height=\"224\" src='data/boyzplt.png''>\n",
    "    <figcaption>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8709a63",
   "metadata": {},
   "source": [
    "#### Segmenting the image to 50 segments so we do not end up explaining every pixel.\n",
    "```python\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "segments_slic_pil = slic(pil_img, n_segments=50, compactness=30, sigma=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16111485",
   "metadata": {},
   "source": [
    "#### define a function that depends on a binary mask representing if an image region is hidden\n",
    "\n",
    "```python\n",
    "def mask_image(zs, segmentation, image, background=None):\n",
    "    if background is None:\n",
    "        background = image.mean((0,1))\n",
    "    out = np.zeros((zs.shape[0], image.shape[0], image.shape[1], image.shape[2]))\n",
    "    for i in range(zs.shape[0]):\n",
    "        out[i,:,:,:] = image\n",
    "        for j in range(zs.shape[1]):\n",
    "            if zs[i,j] == 0:\n",
    "                out[i][segmentation == j,:] = background\n",
    "    return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708ab31",
   "metadata": {},
   "source": [
    "#### Creating a predictor that works on super-pixels as opposed to the original image.\n",
    "\n",
    "```python\n",
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG15()\n",
    "def f(z):\n",
    "    print(\"inside f\")\n",
    "    return model.predict(preprocess_input(mask_image(z, segments_slic, pil_img, 255)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9939be",
   "metadata": {},
   "source": [
    "#### Creating an explainer using shap library\n",
    "```python\n",
    "import shap\n",
    "explainer = shap.KernelExplainer(f, np.zeros((1,50)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c0d88",
   "metadata": {},
   "source": [
    "#### Computing Shaply values using our explainer. \n",
    "We run VGG 100 times on the perturbed dataset.\n",
    "```python\n",
    "shap_values = explainer.shap_values(np.ones((1,50)), nsamples=1000) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147882d",
   "metadata": {},
   "source": [
    "#### Visualization of the explanation\n",
    "In explaining how the white dog has been identified as a west highland terrier, we can see the face (colour green) and the positive contribution of 0.1 has the highest impact. Notably we can also observe that the red zones have the most negative imact.\n",
    "\n",
    "<figure>\n",
    "    <img img align=\"center\"  \" src='data/boyzexplained.png'>\n",
    "    <figcaption>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823dd25",
   "metadata": {},
   "source": [
    "You can find the full code [here](SHAPExample.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe69f3",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "1. https://arxiv.org/pdf/1602.04938v1.pdf\n",
    "2. https://arxiv.org/pdf/2011.07876.pdf\n",
    "3. https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\n",
    "4. https://github.com/marcotcr/lime/tree/master/doc/notebooks\n",
    "5. https://arxiv.org/pdf/1705.07874.pdf\n",
    "6. https://vknight.org/Year_3_game_theory_course/Content/Chapter_16_Cooperative_games/\n",
    "7. https://www.rand.org/content/dam/rand/pubs/papers/2021/P295.pdf\n",
    "8. https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Mikroökonomik/Cooperative_game_theory/B1_gl.pdf\n",
    "9. https://www.youtube.com/watch?v=9OFMRiAVH-w\n",
    "10 https://arxiv.org/pdf/1705.07874.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
