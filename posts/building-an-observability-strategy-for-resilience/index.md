---
title: "Building an Observability Strategy for Resilience"
description: "Learn how to build a comprehensive view of your workload, starting from the customer experience and diving down into application logs, to improve your resilience posture. We define observability, monitoring, and resilience before unpacking the tightly coupled relationship between these three topics."
tags:
    - aws
    - observability
    - resilience
    - devops
authorGithubAlias: khubyar
authorName: Khubyar Behramsha
date: 2023-09-20
showInHomeFeed: true
---

## Introduction

It is 4:25pm on a Friday afternoon, you're a software engineer, and just minutes before you shut down for the weekend, you get a call saying that a customer can’t complete their order on your website. Immediately, following a groan of despair, you start investigating the issue and start to ask around if there are any issues with your system, but you are having trouble understanding if this is a platform-wide issue or something isolated to one customer. Soon, you find out that parts of your order processing service are down and multiple customers are impacted, but not all. Questions start to come down from management about what exactly is impacted, what is the level of impact, how much longer can we sustain increased load on the remaining infrastructure, and finally, should we engage our disaster recovery plan? 

Building resilient systems is a high priority for business and technology teams intent on delivering a positive customer experience. Designing these systems requires the collaboration of many teams, a data-driven decision-making process, appropriate technology support, and a centralized strategy across the organization. In this blog we will start by defining some key terms. Then, we break down the different phases in buliding an observability strategy. Finally, we'll share guidance and best practices to help you build your own observability strategy. Throughout, we'll use the scenario above to provide examples of what you might eventually build for your own workloads. While the below covers general concepts that can be applied to workloads running anywhere, we will specifically be focusing on workloads running in the cloud.

## Definitions to know

Let's start by aligning on definitions for terms we'll use throughout this blog: metrics, monitoring, observability, and resilience. [Metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html) are data about the performance of your systems. Monitoring is the systematic collection of metrics. In modern control system theory, observability is defined as the ability to determine the internal states of the system by observing its output. Resilience is the ability of an application to resist or recover from certain types of faults or load spikes, and remain functional from the customer perspective. Monitoring enables observability which enables us to acheive our goal, improving the resilience of our systems and better serve our customers.

## Defining resilience objectives, SLIs and establishing a baseline

Before planning any journey, its typically important to know where you are starting from and where you want to go. Things are no different here. To start building an observability strategy it's important to know what we are working toward, and that is where having well-defined resilience objectives comes in. [Resilience objectives](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/business-continuity-plan-bcp.html#recovery-objectives-rto-and-rpo) are commonly measured in terms of recovery point objective (RPO) and recovery time objective (RTO), or simply, "how much data can you lose?" and "how long can your service be disrupted?". Establishing these requires collaboration between technology and business teams and a discussion on tradeoffs between complexity, cost, and resources required to meet the objectives. Once this is in place, you have a common target that all teams can work toward. 

To aid in the RPO/RTO discussion, start by preparing yourself with information on the current state of your workload. Gathering details about the current workload architecture, the duration and cause of past incidents, current performance benchmarks, and historical trend data from metrics you are already monitoring will allow you to have a data-driven conversation with the business when establishing objectives. Though objectives will differ from workload to workload, they commonly fall into one of four categories, RPO/RTO of: hours, tens of minutes, minutes, and real or near real-time. 

In addition to capturing what you already are doing, you also want to establish the key indicators you will use to identify the health of your system, often referred to as service level indicators or SLIs. It is important to select not only the right SLIs based on the things your users expect from the system, but also the right amount of these. For example, if your application's primary purpose is storing data for customers, use indicators of durability or successful data uploads. On the other hand, if you've got a system that presents data to customers in real-time, page load times and error rates might be better indicators of system health. Choosing too few SLIs puts you at risk of leaving large parts of the system unexamined, while selecting too many can create unnecessary noise leading to alarm fatigue and distractions from the most critical indicators.

Our observability strategy now includes resilience objectives (where we want to go), a baseline of metrics for our workload (where we're starting from), and SLIs we'll be tracking (the trail markers along our path). 

## Observability into the customer experience, operations, and failure scenarios

Now that we've established where we are and where we want to go, we need to plan the best route possible. Here, we'll continue building the strategy by establishing the monitoring we need to implement. Most systems have several personas and teams involved, each accountable for their own function and concerned with metrics they are responsible for. Database engineers might focus on load on the database and the number of concurrent sessions. Developers would care about application errors being thrown and users being unable to login to the system. Operations engineers look at available disk space and CPU utilization. Business owners are interested in observing the number of orders placed or new user sign-ups. While all these might individually be important metrics to monitor, each group focusing on their own silos can lead us lose sight of the overall picture. Thankfully, there is one thing that all parties involved are accountable for as a common goal, the customer or user experience. When defining what monitoring is needed, start with the metrics that measure the user experience. Whether your "users" are humans or other services, consider starting by measuring latency, traffic, errors and service capacity. So, in addition to monitoring actual user metrics, it will be helpful if we can simulate customer actions using synthetic canaries and simulated customer load. This provides a steady stream of data points that we can measure against and also allows us the ability to test, in a non-production environment, the impact of changes to our system from things like code deployments, infrastructure modifications, new architecture designs. 

While monitoring simulated and live user metrics provides a good representation of the customer experience, these are often lagging indicators. Ideally, to improve our overall resilience posture we can identify leading indicators at our application and infrastructure level that could cause downstream impacts to the customer. When building an observability strategy, there will be some standard metrics and best practices you can safely recommend all workloads implement, and others that will be specific to that workload. The strategy should be focused on the outcomes of implementing the appropriate telemetry so that individual teams have the flexibility to develop what they need, but still meet your organization's resilience objectives. Setting hard requirements like "CPU utilization should always be less than 85%" may not apply to workloads that use serverless services. Instead, recommending that system capacity should stay 15% higher than system load, allows teams across the organization to monitor their systems and respond in a similar way even though the underlying infrastructure looks different.

Now that the strategy has addressed customer and performance metrics, the focus can shift to failure scenarios. These are the common modes of failure that any workload can have. Think server restart or termination, network firewall outage, AZ (availability zone) disruption, or one of the most common, a bad deployment. These known factors that will have a negative impact on the workload and are especially important to look out for. While some of these may seem obvious, others might not be as apparent. Reviews with of a workload following [Well-Architected best practices](https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html) allow you to leverage the expertise, experience, and sometimes tribal knowledge of all team members associated with the workload to uncover failure modes that could been overlooked. When accounting for the different failure scenarios, do not forget to own your dependencies! Things your workload relies on that fall outside your direct control can still impact the workload, and therefore become part of your responsibility. Where loosely coupling dependent systems is not an option, focus on setting up the right monitoring to provide notice as early as possible of any potential disruptions. For an organization with a well-defined observability strategy for resilience, this becomes much easier as you can leverage telemetry already developed by the teams of the systems yours depends on.

## Observability for post-incident retrospectives

An important part to maintaining resilient systems is ensuring that issues that do occur, only do so once. As the Amazon CTO says, "Everything fails, all the time". No matter how much pre-work you do and how much time you spend on building out the perfect strategy, there are always things that were not considered. What is important is to learn from every incident, good or bad, and continue to improve the observability of your systems. An important piece of being able to perform incident retrospectives and root cause analyses is having access to the appropriate logs. Consider defining common logging methods and minimum requirements across the org to standardize the way information is gathered and made available. If during an incident you realize it took too long to identify the root cause, or that an alarm that you were expecting to be triggered never did, this is the time to close the loop and implement whatever is needed to prevent the issue from reoccurring.

## Conclusion

Now picture the same scenario that we started with, this time with the appropriate observability measures in place. Starting at 2:00 pm, you get warnings of increased error rates on your order processing applications. Support engineers swarm to investigate and resolve this issue while customer service prepares potential communications for customers. Senior management is alerted early that there’s an ongoing issue but that 90% of customer’s are without impact, and ones impacted are only receiving intermittent errors. Management is now closely monitoring this metric to see if it reaches a critical point and the failover plan needs to be executed. Thankfully, you’ve caught the problem early on and identified an underlying infrastructure failure. The troublesome infrastructure has been replaced and requests error rates drop to a healthy level by 3:15 pm. Customer impact was minimized and your teams get to go home early and enjoy the weekend!

Thanks to having a well-defined observability strategy you had the data you needed to make critical decisions, were aware in near real-time of the status of your service and what customers were experiencing, and finally able to quickly identify the cause of the problem and mitigate the impact. Following the event, you also have data needed to complete a retrospective of the incident, perform a root-cause analysis and build your correction of error plan to avoid this happening again.
