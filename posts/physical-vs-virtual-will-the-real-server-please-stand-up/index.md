---
layout: blog.11ty.js
title: Physical vs Virtual — Will the ‘real’ server please stand up!
description: Article that explains how today's computing landscape is shaped by the types of servers used for traditional systems, and modern applications that run in the cloud. 
tags:
  - servers 
  - hypervisor, 
  - VM, 
  - virtual machine, 
  - hardware, 
  - compute, 
  - bare metal
authorGithubAlias: cie247
authorName: Curtis Evans
date: 2022-09-19
---

***Spoiler Alert!***...Technically, there is no ‘*real*’ difference between physical and virtual servers. All computer systems, regardless of category require hardware components to provide power, memory, storage and other processing capabilities needed so that code deployed on it can run applications. What is different is the way in which the various hardware and software configurations are implemented that classify the system as a physical or virtual server. 

Now that this revelation has been made at the top of the article, I hope you are not deterred from reading on. Stick around, and I will assure you that there’s valuable insight to learn about the difference between server types, why we use one vs the other , and most importantly which option is best for your compute needs. Before we move on, be mindful that throughout this article the terms ‘*server’*, ‘*system’* and ‘*machine’* are used interchangeably; they all mean the same thing. 

What is a *physical server*? ** A physical server is a system that is equipped with a motherboard, CPU, memory, hard drive, keyboard, monitor and one or more controllers for managing various tasks. In today’s world of server technologies where cloud computing is omnipresent, it is common for physical servers to be referred to as “*bare-metal*” systems. That is because the hardware functions with just a single operating system like Linux, or Windows and generally belongs to a particular user or company often referred to as a “tenant”. Another way to think about physical servers is if a programming bug, slow network connection, or software application crash frustrates you so badly that you can actually “*kick it”* you are working with a physical server!

By comparison, a *virtual server* can be described as a software implementation of a physical server. That is a virtual server (*virtual machine*, or *VM* as it is commonly called) also requires hardware as mentioned earlier, however the CPU, RAM, and I/O controller devices are abstracted away from the running mode. This abstraction allows components to be shared by multiple VMs which turns one physical computer into many computers. Using the frustrated user analogy from earlier, if one VM is having problems, kicking the computer may result in doing harm to the other VMs that were operating just fine!

Not completely satisfied with the difference between the two types of systems? Do you still question if there is a difference at all now that you know the hardware is basically the same? Let’s discuss the *hypervisor* then. I mentioned previously that for virtual machines the hardware is abstracted from the running mode. This is possible because of the role of the hypervisor in the system. The hypervisor is software or firmware that supplants the operating system to create a layer of isolation that partitions the hardware components into controllable units. Physical machines do not use hypervisors because it is not required; the OS is designed to communicate directly with the hardware. It is common for more than one VM to run on the same physical hardware sharing resources, but use different operating systems and application stacks that run independent of each other. This concept is even leading to additional resource abstraction that is giving rise to the use of Container technologies as well as event or function-based compute services like [AWS Lambda](https://aws.amazon.com/lambda/). Again, this is all made possible in large part to the hypervisor. The following diagram provides a basic illustration of the layers inside of a physical server and a virtual server.

![Block diagram showing the layers of components for physical machines and virtual machines](images/physical-vs-virtual1.jpg "Server Types")

Believe it or not, there was a time when virtual machines did not exist and running physical servers was the only option. An argument can be made that mainframe systems and the concept of time-sharing which spreads compute resources amongst users was an early form of virtualization. For this article, we will discuss virtualization in the context of modern systems. 

Walking inside of a loud, very cold data center you’re liable to find rows of server racks with x86 or ARM-based machines that are dedicated to a single task. Historically, users, programmers, SysAdmins and IT Operations staff were totally fine with this simplicity, because if a problem occurred with a machine, the attention of troubleshooting would be focused on that particular system without impact to the other machines. Granted, you did not want to be the customer or line of business whose machine failed because that meant you had to wait for it to be restored or worst case replaced before it was available again.

For many, this approach to computing inside the data center was an acceptable trade-off, but not so for the Finance department back at headquarters. Running these single, isolated ‘*do-one-thing*’ machines became very expensive. In addition to the high cost of equipment, software licensing, power and floor space inside the data center, it was also discovered that utilization for these systems was  below optimal. In other words, a system configured with 32 GB of RAM was only consuming half the memory, and hard drives with terabytes of storage capacity sat unused. Operating costs could be justified during peak business hours when users accessed the systems frequently to run reports, make purchases, and perform other tasks. But what about the drop-off in activity during the evening and weekends? It was clear to the business that tons of money was being wasted during off-peak hours which meant these high cost, low-utilization problems needed a solution. The answer would lie in the birth of virtualization. 

When virtualization arrived on the scene, the benefits were immediate. The ability to consolidate system resources and reduce the number of servers throughout the enterprise meant that twenty physical machines that once took up valuable real estate in the data center could now be replaced with 1 or 2 machines using hypervisors to run 20 virtual machines instead. This concept certainly excited finance teams across many companies! Other benefits of virtualization include the reduced time required to perform operations and manage environments faster and more efficiently. Migrating legacy workloads, performing application failovers, or backing up system data for disaster recovery (DR) proved to be much simpler to implement. As software vendors began introducing more advanced features into its virtualization technology for live-migration of workloads, this gave SysAdmins and IT Operations staff the ability to replace failed hardware, add operating systems and software to existing machines, and distribute workloads evenly across systems with minimal disruption. This flexibility would go a long way in eliminating bottlenecks and reducing outage times for users who used to be severely impacted for hours or days at a time. 

Back to costs for a second...I would like to point out that investment costs required to move from physical servers to virtualization cannot be overlooked. Acquiring new hardware, licensing, technical support for hypervisors, training, and educating staff adds up. In the long run however, these up-front expenses can yield a significant return-on-investment gained through reduced hardware footprints, lower energy costs for power & cooling, plus other savings that come with running a lean, efficient IT operation. From a revenue standpoint, there’s also the increased financial benefit realized for companies. A good example is online retail where customers now purchase items 24x7 with fewer issues accessing web sites because companies that use virtual servers to run their digital storefronts are better positioned to keep the systems driving e-commerce more reliable.

Virtualization is also the defacto mode for services running in public cloud environments. For the past 15 years, there has been a huge surge in the way businesses now consume CPU and memory resources due to abstraction. Some of the largest selling points for public cloud providers is the ability to access services on demand, and deploy workloads around the globe in minutes. Additionally, there’s the added value of scaling resources up or down using the right amount of capacity to meet business demands, and building workloads that are fault tolerant and highly available through recovery from failures instantaneously. These benefits are all possible because public cloud providers use virtualization on just about every server running in their global data centers. And as more abstraction is added, cloud providers can offer more value to customers by off-loading the non-strategic activities that come with hardware management. Being able to support millions of customers means having the flexibility to offer services to match various use cases and application patterns. As a result, companies are increasingly adopting an “all cloud” or “cloud first” approach when building new applications. Other organizations that choose to take a balanced approach with hybrid environments (a mix of on-prem and cloud resources), can rely on their existing data centers as normal while extending operations to public cloud where practical. As demand for system access and applications continues to grow, companies can add more resource capacity when needed, and spin it down when no longer required to manage costs. Using the online retail example again, if a clothing company is anticipating an increase in online shoppers during the holiday season, instead of adding more hardware to the data center they can leverage cloud resources instead to handle the spike in traffic. Even when traffic increases unexpectedly, the same approach can be applied.

*If virtual servers are so great, why do I even need physical machines anymore?*   
Despite the tremendous benefits of virtualization, there are existing use cases that justify the need for bare metal. The most common being those workloads that simply cannot be virtualized. Those include legacy business applications originally designed to run on commercial UNIX, AS/400 or mainframe platforms (yes, these still exist today and for most run just fine!). Other examples are those applications that require specialized hardware available on a physical server to maximize performance. An example would be a post-production operation at a major film studio that’s putting the final touches on scenes for an epic science-fiction movie. These types of workloads require lots of graphics processing and special effects rendering to deliver the highest of audio and video quality. It would be difficult for a studio to pass up full use of a server with 64 CPU cores, 4 NVIDIA GPUs, 32 TBs of RAM, and 200+ TBs of local scratch storage in order to get the project completed on time. As virtualization and the ability to offer high-end performance in the cloud improves, expect to see advancements being made in software as companies will work with cloud providers and hypervisor vendors to bring those performance capabilities to VMs on-demand.

>identify a “potential” call to action (e.g., have them better understand where VMs vs Physical machines make sense in their environments)
